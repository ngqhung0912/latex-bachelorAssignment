Artificial Neural Network (ANN) is a set of algorithms that seeks to identify correlations in data utilizing a technique inspired by how the human brain operates - mimicking how each neuron in the brain signals each other. The most basic ANN model is the Feed-forward Multilayer Perceptron Neural Network (MLPNN), in which the purpose is to define the mapping between the input and output \(y = f(x;\theta)\) and approximate the parameter \(\theta\) which results in the best possible function. In MLPNN, the data will flows in one direction from the input to the output, hence the name feed-forward. Like other supervised learning algorithms, an MLPNN needs to be trained before accurately describing the input and output relations. This is typically done by feeding the network with pre-labeled data, comparing the model's output with the desired output, and updating the weights parameter \(\theta\) - a process called backpropagation. In this assignment's context, Neural Network (NN) will be used when referring to Feedforward Multilayer Perceptron Neural Network, and NN models implemented in this research are provided by the open-source library \texttt{TensorFlow}~\cite{tensorflow2015-whitepaper}.

\subsubsection{Designing a Neural Network}

There are no general rules for determining the number of layers and the number of neurons per layer, and it depends heavily on each use case. While Benvenuti et al.~\cite{nn-calibration}, He et al.~\cite{NN-GA}, and Daniel et al.~\cite{NN-coarse} used only a single-layer ANN and varied the number of neurons, Ye et al.~\cite{YE2019292} vary both. However, the ultimate goal in both case is to find the combinations which result in the minimum error while also avoiding overfitting, i.e., the model excels on training but perform poorly on the validation step. For each simulation material, 250 models ranging from 2 to 15 layers and 5 to 15 neurons per layer are tested to determine the best model. Each model is trained for 50 epochs with a batch size of 32, and the metric used to grade the model is the Mean Absolute Error. The optimization algorithm used is Adam~\cite{adam}, which is easy to configure, combines the best features of other optimization algorithms, and works robustly in most cases. In addition, 20\% of the data will be saved to validate the model. 

Another important component of a Neural Network is the activation function. Since each neuron performs calculation by multiplying the input with weight and adding a bias, the activation function's role would be introducing a non-linearity element into an otherwise linear neuron. According to Goodfellow et al., ~\cite{DL-Goodfellow}, Rectified Linear Unit (ReLU) is the recommendation for most Deep Learning models, with its ability to preserve much of the properties due to its near-linear shape. ReLU activation function is defined as $f(x) = max(0, x)$. 

